{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Neural Networks \n",
    "This is my final project for the Computer Vision class thought by Rob Fergus in Fall 2016. I got the project of implementing compressing ideas of Song Han presented in paper `Learning both Weights and Connections for Efficient Neural Networks`. First I will go over the paper and present a summary. Then I will implement the ideas in Torch7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Connections for Efficient Neural Networks by Song Han\n",
    "Song Han starts the paper with a focus on energy consumption of neural networks and motivates the need of compressing neural networks by pointing out that a smaller network would fit in memory and therefore the energy consumption would be less. He first talks about the Related Work\n",
    "##### Related Work\n",
    "- He first points out several quantization and low-rank approximation methods and says that those methods are still valid and can be applied after network pruning.[11-12-13-14]\n",
    "- Then he mentions two methods utilizing global average pooling instead of the fully connected layer. I think this idea follows the fact that most parameters in popular models are due to the fully connected layers. [15-16]\n",
    "- Then he mentions the early pruning ideas e.g. biased weight [17] decay and some others [18-19]\n",
    "- Lastly the Hashed Nets are mentioned and an anticipation about pruning may help it presented[20]\n",
    "\n",
    "#### Learning Connections in Addition to Weights\n",
    "To prune the network importance of each weight is learned with a training method that is not mentioned. After this learning step, connections whose importance weights below a certion threshold are removed. Then the network is retrained and this part is crucial. \n",
    "- __Regularization__: L1 and L2 regularizations are used during training and it is observed that even though L1(forces sparse weights) gives better after pruning accuracy, the accuracy after retraining observed to be better with L2\n",
    "- __Dropout Factor__: Dropout should be adjusted during retraining due to the pruned connections with $D_{new}=D_{old}\\sqrt{\\frac{C_{new}}{C_{old}}}$\n",
    "- __Local Pruning and Parameter Co-adaptation__: No reinitilization is made after pruning because it is obviously stupid to do. You can just train with smaller size if that was possible. ConvNet part and Fully Connected Layer's are pruned separetely. It is mentioned that computation is less then original since partial retraining is made. _I am not sure why this is not made layer by layer if vanishing gradients are the problem._\n",
    "- __Iterative Prunning__: Prunning made through iterating and doing _greedy search_ at each iteration. I am not sure which algorithm _greedy search_ supposed to point here. It is a vague term. Doing one pass agressive prunning led bad accuracy. \n",
    "  My guesses about about iterative _greedy search_ are:\n",
    "  - Choose threshold iteratively. \n",
    "  - ?\n",
    "\n",
    "\n",
    "- __Pruning Neurons__ Neurons are removed if there are either no incoming or no outgoing connections before retraining.\n",
    "\n",
    "#### Experiments\n",
    "Caffee is used and a mask is implemented over the weights such that it disregards the masked outparameters.\n",
    "![pruning](pruning.jpg)\n",
    "- __Lenet5__: After pruning retrained with LR/10 and a nice visualization is provided to show the outside weight are pruned(attention) \n",
    "  [ ] Check: weights and importance weights give simalar plot like this.\n",
    "- __Alex-Net__: 73h to train on NVDIA Titan X GPU. 173h to retrain with LR/100.\n",
    "- __VGG__: Most of the reduction is at fully connected layer.\n",
    "\n",
    "#### Discussion\n",
    "There is still one point that is not clear to me how the pruning is made with L1 or L2. I need to think about this. But basically in this section it is shown that iterative prunning with L2-regularization gave best results. One need to prune different regions separetely. Because FC layers are more prunable. \n",
    "- Layers are pruned layer by layer. Sensitivity increases with deepness of the layer. It is not mentioned but the reasn might be that the initial results effect more results and it may propogate and increase! \n",
    "- Each layer's sensitivity is used as threshold to prune each layer.\n",
    "- Pruned layers are stored as a sparse matrix (a overhead of 15%, how!?).\n",
    "- Weight distribution of before/after pruning is given. Weights around 0 is disapeared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- After reading the paper there is one concern left, how do you learn the the importance of the connections. What does gready search means. This is a question that I need to address. \n",
    "    - Maybe you can define a new network. weights*input being input and importance weights maps everythingto the output. Lets start with trying this!\n",
    "    - Lets look read the other papers and learn how to do a _greedy search_\n",
    "- Implement 2 Lenet on mnist and masking\n",
    "    - Implement this and train!\n",
    "    - Implement mask and do it with just weight threshold and see the difference. (new package called from main.lua\n",
    "    - Implement retraining \n",
    "- Implement importance learning\n",
    "- Output the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
