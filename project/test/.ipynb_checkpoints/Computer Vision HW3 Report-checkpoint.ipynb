{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision HW3 Submission\n",
    "__Name__: Utku Evci  \n",
    "__NETid__: ue225  \n",
    "__Date__: 12.10.2016 \n",
    "\n",
    "Over one week period I have experimented with different models on HPC. I've thought that limited GPU's are going to be pretty busy and therefore implement my code to work with cpu in a multithread matter. I've started by implementing preprocessing separete then the training, such that I don't preprocess same image twice. I've implemented ParallelIterator, however realized that it is not required when you already preprocessed the data and just reading from ram. Therefore haven't needed it for my experiments. I've follow a similar approach to organize my code `models/` file includes the models I have used and `prepro/` folder includes the preprocesing methods I've experimented. I've provided the convergence graphs of the models and the operation breakdowns with each model. I've used [Lua-profiler] to approximate #operations per model. (https://github.com/e-lab/Torch7-profiling/) Since no particular report format is specified I will follow an incremental manner to tell my story about the homework. It was a fun homework and thanks for the class, I really liked it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar and basic32\n",
    "![im1](bds/cifar.jpg)\n",
    "\n",
    "This is the default model given, I've trained 200 epochs with the default parameters (LR=0.1, momentum=0.9). As I introduced above I've implemented preprocessing as a separete process and named the default basic processing(given) as `basic32`. The algorithm converges quickly, therefore I've plot only the first 25 epoch. Even though the error in both case approaches zero, the test error approaches to 5%. The reason for that might the fact that the dataset allready includes multiple version(scaled,transformed) of the same traffic sign. We are getting the validation data randomly from training data and therefore validation data becomes something quite similar to training data itself. In other words our validation data is sampled from training data not from the real world, a better approach needs to set the validation data wisely (without having same image in both datasets).\n",
    "![im1](c_plots/9267369.png)\n",
    "\n",
    "- **Learning Rate** After my first trials I've wanted to play with learning rate and see its effects. I've change the learning rate up(0.5) and down(0.01). The learning rate seemed to be doesn't necessearly effect the final output, if you increase it a lot though, it doesn't converge. However 0.1 seemed to be an appropriate learning rate to me, because when LR=0.01 converge quite slow, but it looks like it has better generalization but converges slowly. \n",
    "\n",
    "0.1 | 0.5 | 0.01\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267369.png) | ![im1](c_plots/9270523.png) | ![im1](c_plots/9268686.png)\n",
    "\n",
    "- **Resampling Data** Second I wanted to implement resampling. Since I decided to preprocess the data separete from the training I've padded the training data with more samples from each class which has less sample then the class who has maximum samples. At the end the size of the data set increased to 96750 from 39209. I've observed an even faster convergence(probably due to increased training-dataset size).  \n",
    "\n",
    "![im1](c_plots/9267393.png)\n",
    "\n",
    "- **Changing Momentum** Third I wanted to see efect of momentum. I believe that momentum really helps the model to converge properly. But it doesn't increase the final accuracy enourmously.\n",
    "\n",
    "momentum = 0.95 | momentum = 0.75\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267402.png) | ![im1](c_plots/9267409.png) \n",
    "\n",
    "- **Multihreading / ParallelIterator** I have realized that the original code, preprocesses the images during training at each epoch again and therefore creates a bottleneck. Instead I decided to process the images before training and save it, then read it at each epoch with the iterator. By doing so we are processing the data-set only once, and the iterator only reads/shuffles/do the partition of the data. I've implemented parallel-iterator and used `nThrLoad` flag to define multiple parallel iterators. However I've observed that this reduced the performance and increaased the memory usage considerably. I've observed that one thread is fast enough at serving multiple threads(12-16) and therefore stick with the single iterator for the rest of the trainings. \n",
    "\n",
    "- **BEST_RESULT: ** around 95% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv1\n",
    "![im1](bds/conv1.jpg)\n",
    "\n",
    "I wanted to see what happens if I decrease the number of filters in the Conv layer. This choice does make a big difference in terms of total number of parameters, however #operations are around 3 times less. So I've trained this model with 0.1 learning rate two times: one with other without resampling. The test accucary dropped drasctically to 90%. Resampling provided %0.5 increase in the accucary. \n",
    "\n",
    "w/ resampling | w/o resampling\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267625.png) | ![im1](c_plots/9267615.png) \n",
    "\n",
    "- **BEST_RESULT: ** around 90.4% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv2/3 \n",
    "conv2 | conv3\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](bds/conv2.jpg) | ![im1](bds/conv3.jpg)\n",
    "![im1](c_plots/9268219.png) | ![im1](c_plots/9268232.png) \n",
    "\n",
    "So it gets worst if I make the #filters, why not increasing them. I’ve created 2 new models `conv2` having double amount of filters and one extra of fully connected layer. I’ve trained the model and I got pretty much the same result as the base-cifar model gets. `conv3` was an experiment making the #filters of the first layer big. I’ve read that it is better if it is a small number and you increase number of filters while decreasing the dimensions through conv-layers. I wanted to check that and the result confirmed the statement. I’ve got an error around %5.6, which is .6% worst then the base-cifar model. \n",
    "\n",
    "- **BEST_RESULT: ** around 95% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv4/5/6 and basic64 \n",
    "conv4 | conv5 | conv6\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](bds/conv4.jpg) | ![im1](bds/conv5.jpg)  | ![im1](bds/conv6.jpg)\n",
    "![im1](c_plots/9270868.png) | ![im1](c_plots/9270989.png) | ![im1](c_plots/9270997.png) \n",
    "\n",
    "So at this point I decided to increase the input-image size and start scaling images to 64x64 and I've introduced model `conv4` with it. It does increased my best score by 1%. It was considerably slower then the base model `cifar`(10x). Therefore I wanted to measure whether the increase was due to the increased #filters or due to the dropout layer. I've created two models while decreasing the filter sizes and making the models more plausable to train. Even though the graphs are not informative enough due to bath partition between train and validation test, I've got following test scores for the three model for 30 epochs. \n",
    "\n",
    "conv4 | conv5 | conv6\n",
    ":-:|:-:|:-:\n",
    "4.13% | 8.3% | 3.3 % \n",
    "\n",
    "Dropout appeared as being quite important for better accuracy. With the better and faster model `conv6` I've run 100 epoch and got 2.9%\n",
    "- **BEST_RESULT: ** around 97.1% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv7 and norm64\n",
    "![im1](bds/conv7.jpg) \n",
    "\n",
    "Then I've implemented couple of suggested normalization method. First I transformed the images to YUV space and then globally normalized them such that each channel has zero mean and unit variance. I've also extented `conv6` model with a starting Spatial Contrastive Normalization layer and named the new model suprizingly as `conv7`. I haven't obverved any significant difference between `norm64`(3.2%) and `basic64`(3%) preprocessing scripts. I've start observing a n osccillation at validation error at this point and start thinking of implementing adaptive learning rate.\n",
    "\n",
    "conv6/basic64 | conv6/norm64 \n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9275422.png) | ![im1](c_plots/9275084.png) \n",
    "\n",
    "However `conv7` gave superior result to `conv6` and increased my best result to %2.6\n",
    "\n",
    "conv7/basic64 | conv7/norm64 \n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9277101.png) | ![im1](c_plots/9277119.png) \n",
    "\n",
    "- **BEST_RESULT: ** around 95% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# conv48-1/2/3 and cut48/cut48contra\n",
    "conv48-1 | conv48-2 | conv48-3\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "![im1](bds/conv48-1.jpg) | ![im1](bds/conv48-2.jpg)| ![im1](bds/conv48-3.jpg)\n",
    "![im1](c_plots/9268219.png) | ![im1](c_plots/9268232.png) \n",
    "\n",
    "So it gets worst if I make the #filters, why not increasing them. I’ve created 2 new models `conv2` having double amount of filters and one extra of fully connected layer. I’ve trained the model and I got pretty much the same result as the base-cifar model gets. `conv3` was an experiment making the #filters of the first layer big. I’ve read that it is better if it is a small number and you increase number of filters while decreasing the dimensions through conv-layers. I wanted to check that and the result confirmed the statement. I’ve got an error around %5.6, which is .6% worst then the base-cifar model. \n",
    "\n",
    "- **BEST_RESULT: ** around 95% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
