{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Neural Networks \n",
    "This is my final project for the Computer Vision class thought by Rob Fergus in Fall 2016. I got the project of implementing compressing ideas of Song Han presented in paper `Learning both Weights and Connections for Efficient Neural Networks`. First I will go over the paper and present a summary. Then I will implement the ideas in Torch7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Connections for Efficient Neural Networks by Song Han\n",
    "Song Han starts the paper with a focus on energy consumption of neural networks and motivates the need of compressing neural networks by pointing out that a smaller network would fit in memory and therefore the energy consumption would be less. He first talks about the Related Work\n",
    "##### Related Work\n",
    "- He first points out several quantization and low-rank approximation methods and says that those methods are still valid and can be applied after network pruning.[11-12-13-14]\n",
    "- Then he mentions two methods utilizing global average pooling instead of the fully connected layer. I think this idea follows the fact that most parameters in popular models are due to the fully connected layers. [15-16]\n",
    "- Then he mentions the early pruning ideas e.g. biased weight [17] decay and some others [18-19]\n",
    "- Lastly the Hashed Nets are mentioned and an anticipation about pruning may help it presented[20]\n",
    "\n",
    "#### Learning Connections in Addition to Weights\n",
    "To prune the network importance of each weight is learned with a training method that is not mentioned. After this learning step, connections whose importance weights below a certion threshold are removed. Then the network is retrained and this part is crucial. \n",
    "- __Regularization__: L1 and L2 regularizations are used during training and it is observed that even though L1(forces sparse weights) gives better after pruning accuracy, the accuracy after retraining observed to be better with L2\n",
    "- __Dropout Factor__: Dropout should be adjusted during retraining due to the pruned connections with $D_{new}=D_{old}\\sqrt{\\frac{C_{new}}{C_{old}}}$\n",
    "- __Local Pruning and Parameter Co-adaptation__: No reinitilization is made after pruning because it is obviously stupid to do. You can just train with smaller size if that was possible. ConvNet part and Fully Connected Layer's are pruned separetely. It is mentioned that computation is less then original since partial retraining is made. _I am not sure why this is not made layer by layer if vanishing gradients are the problem._\n",
    "- __Iterative Prunning__: Prunning made through iterating and doing _greedy search_ at each iteration. I am not sure which algorithm _greedy search_ supposed to point here. It is a vague term. Doing one pass agressive prunning led bad accuracy. \n",
    "  My guesses about about iterative _greedy search_ are:\n",
    "  - Choose threshold iteratively. \n",
    "  - ?\n",
    "\n",
    "\n",
    "- __Pruning Neurons__ Neurons are removed if there are either no incoming or no outgoing connections before retraining.\n",
    "\n",
    "#### Experiments\n",
    "Caffee is used and a mask is implemented over the weights such that it disregards the masked outparameters.\n",
    "![pruning](pruning.jpg)\n",
    "- __Lenet5__: After pruning retrained with LR/10 and a nice visualization is provided to show the outside weight are pruned(attention) \n",
    "  [ ] Check: weights and importance weights give simalar plot like this.\n",
    "- __Alex-Net__: 73h to train on NVDIA Titan X GPU. 173h to retrain with LR/100.\n",
    "- __VGG__: Most of the reduction is at fully connected layer.\n",
    "\n",
    "#### Discussion\n",
    "There is still one point that is not clear to me how the pruning is made with L1 or L2. I need to think about this. But basically in this section it is shown that iterative prunning with L2-regularization gave best results. One need to prune different regions separetely. Because FC layers are more prunable. \n",
    "- Layers are pruned layer by layer. Sensitivity increases with deepness of the layer. It is not mentioned but the reasn might be that the initial results effect more results and it may propogate and increase! \n",
    "- Each layer's sensitivity is used as threshold to prune each layer.\n",
    "- Pruned layers are stored as a sparse matrix (a overhead of 15%, how!?).\n",
    "- Weight distribution of before/after pruning is given. Weights around 0 is disapeared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- After reading the paper there is one concern left, how do you learn the the importance of the connections. What does gready search means. This is a question that I need to address. \n",
    "    - Maybe you can define a new network. weights*input being input and importance weights maps everythingto the output. Lets start with trying this!\n",
    "    - Lets look read the other papers and learn how to do a _greedy search_\n",
    "- Implement 2 Lenet on mnist and masking\n",
    "    -  ~~Implement this and train! ~~\n",
    "    - Implement threshold picking and generate plot data. w/o retrain for each layer percentage vs accuracy and with retrain\n",
    "    - generate train.lua and main.lua(for pruning only)\n",
    "    - For retraining probably first decide the epoch number.\n",
    "    - ~~Implement mask and do it with just weight threshold and see the difference. (new package called from main.lua ~~\n",
    "    -  ~~Implement retraining ~~\n",
    "- Add Output plots\n",
    "- Implement importance learning\n",
    "    - Idea is get a percentage and for each weight calculate the l2 error if that weight is pruned. Then choose the smallest l-2, repeat this until the percantage requirenment is match. If slow do this with one pass, such that you output intermediate masks. Shouldn't be that long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with first training my first network `LeNet-5` on HPC and got a test error of 96% in 30 Epochs with deafult training parameters. It occupies around 5Mb and has 313k parameters. My goal is to get 10x compression in size following the three methods outlined in the paper. The parameter breakdown is below:\n",
    "![lenet5](lenet5.jpg)\n",
    "\n",
    "### Implementing Network Pruning\n",
    "I wanted to implement every part in Torch. After diving in I realized this might be a hard task. The reason is basically there is no Sparse-Tensor implementation and no space gain is made through making the weigth matrices(connections) sparse. After struggleing a bit, I decided to aim an encoding and decoding method. Because implementing Sparse Tensor's and all the required operations is another project by itself I believe. Layers like SpatialConvolution and Linear is implemented for optimization and source code is not that easy to understand and modify. Therefore I decided to use full weight matrices throughout my experiments and represent connectivity by having non-zero weights.\n",
    "\n",
    "First I've started with `Pruner` module. Pruner module is initilized with `prune` call. `prune` call gets for a mask-generating function(there are two implementations `maskThreshold` and `maskPercentage`), a table of layer-id's and a table of mask-generating function parameters. Basically prune uses the layer-id's to get the weight tensor of target layer. Since this is a development code there are no type-checks and the provided id should be a valid one(a layer with `.weight` field like nn.SpatialConvolution, nn.Linear). Then a mask is generated by calling the provided function with provided parameters and selected weight Tensor. The result is a binary mask with the same size as the weight-Tensor. The mask is saved into the module object. `prune` repeats this proccess for each layer-id and returns the percantage of retained connections for each layer-id. \n",
    "\n",
    "I've played with this and got some initial results by just masking according to the absolute value of the weights and got similar, sometimes better results with around 50% pruning of each layer without retraining. The individual sensitivity of each layer is below.\n",
    "\n",
    "TODO generate plots.\n",
    "\n",
    "\n",
    "Then I've implemented retraining. Since I decided not to create new nn.Module's I have implemented a function returning a function to zero the gradWeight. This function is factorized by the `Pruner` and returned. I've added this function to the hooks.onBakward, so that it is called just before weight update. By doing, so I've implemented pruning by using the original Modules. Since I am using fields and functions of Module class, this approach enabled me to prune any module inherits nn.Module and has a weight-tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
