{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Detection Kaggle Competition\n",
    "\n",
    "In the last assignment of Computer Vision Class at 2016 Fall. We participated in a Kaggle competition with the rest of the class on the [The German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news). The objective was to produce a model that gives the highest possible accuracy on the test portion of this dataset. \n",
    "\n",
    "The benchmark is well researched and there are kwown architectures that gets below 1 percent error. I share in this post my experience with different Conv-Net architectures and Torch on HPC of NYU. \n",
    "\n",
    "Over one week period I have experimented with different models on HPC. I've thought that limited GPU's are going to be pretty busy and therefore implement my code to work with cpu in a multithread matter. I've started by implementing preprocessing separete then the training, such that I don't preprocess same image twice. I've implemented ParallelIterator, however realized that it is not required when you already preprocessed the data and just reading from ram. Therefore haven't needed it for my experiments. I've follow a similar approach to organize my code `models/` file includes the models I have used and `prepro/` folder includes the preprocesing methods I've experimented. I've provided the convergence graphs of the models and the operation breakdowns with each model. I've used [Lua-profiler] to approximate #operations per model. (https://github.com/e-lab/Torch7-profiling/).\n",
    "\n",
    "Source code can be found at https://github.com/evcu/cv2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar and basic32\n",
    "![im1](bds/cifar.jpg)\n",
    "\n",
    "This is the default model given with the starter code. I've trained 200 epochs with the default parameters (LR=0.1, momentum=0.9). As I introduced above I've implemented preprocessing as a separete process and named the default basic processing(given) as `basic32`. The algorithm converges quickly, therefore I've plot only the first 25 epoch.\n",
    "\n",
    "- **Learning Rate** After my first trials I've wanted to play with learning rate and see its effects. I've change the learning rate up(0.5) and down(0.01). The learning rate seemed to be doesn't necessearly effect the final output(I got a slight improvement with (0.5). I also observed that if you increase it a lot, it doesn't converge. However 0.1 seemed to be an appropriate learning rate to me, because when LR=0.01 converge quite slow, but it looks like it has better generalization but converges slowly. \n",
    "\n",
    "0.1 | 0.5 | 0.01\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267369.png) | ![im1](c_plots/9270523.png) | ![im1](c_plots/9268686.png)\n",
    "\n",
    "- **Resampling Data** Second I wanted to implement resampling. Since I decided to preprocess the data separete from the training I've padded the training data with more samples from each class which has less sample then the class who has maximum samples. At the end the size of the data set increased to 96750 from 39209. I've observed an even faster convergence(probably due to increased training-dataset size).  \n",
    "\n",
    "![im1](c_plots/9267393.png)\n",
    "\n",
    "- **Changing Momentum** Third I wanted to see efect of momentum. I believe that momentum really helps the model to converge properly. But it doesn't increase the final accuracy enourmously.\n",
    "\n",
    "momentum = 0.95 | momentum = 0.75\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267402.png) | ![im1](c_plots/9267409.png) \n",
    "\n",
    "- **Multihreading / ParallelIterator** I have realized that the original code, preprocesses the images during training at each epoch again and therefore creates a bottleneck. Instead I decided to process the images before training and save it, then read it at each epoch with the iterator. By doing so we are processing the data-set only once, and the iterator only reads/shuffles/do the partition of the data. I've implemented parallel-iterator and used `nThrLoad` flag to define multiple parallel iterators. However I've observed that this reduced the performance and increaased the memory usage considerably. I've observed that one thread is fast enough at serving multiple threads(12-16) and therefore stick with the single iterator for the rest of the trainings. \n",
    "\n",
    "- **BEST_ERROR: ** around 5% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv1\n",
    "![im1](bds/conv1.jpg)\n",
    "\n",
    "I wanted to see what happens if I decrease the number of filters in the Conv layer. This choice does make a big difference in terms of total number of parameters, however #operations are around 3 times less. So I've trained this model with 0.1 learning rate two times: one with other without resampling. The test accucary dropped drasctically to 90%. Resampling provided %0.5 increase in the accucary. \n",
    "\n",
    "w/ resampling | w/o resampling\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9267625.png) | ![im1](c_plots/9267615.png) \n",
    "\n",
    "- **BEST_ERROR: ** around 9.6% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv2/3 \n",
    "conv2 | conv3\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](bds/conv2.jpg) | ![im1](bds/conv3.jpg)\n",
    "![im1](c_plots/9268219.png) | ![im1](c_plots/9268232.png) \n",
    "\n",
    "So it gets worst if I make the #filters, why not increasing them. I’ve created 2 new models `conv2` having double amount of filters and one extra of fully connected layer. I’ve trained the model and I got pretty much the same result as the base-cifar model gets. `conv3` was an experiment making the #filters of the first layer big. I’ve read that it is better if it is a small number and you increase number of filters while decreasing the dimensions through conv-layers. I wanted to check that and the result confirmed the statement. I’ve got an error around %5.6, which is .6% worst then the base-cifar model. \n",
    "\n",
    "- **BEST_ERROR: ** around 5% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv4/5/6 and basic64 \n",
    "conv4 | conv5 | conv6\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](bds/conv4.jpg) | ![im1](bds/conv5.jpg)  | ![im1](bds/conv6.jpg)\n",
    "![im1](c_plots/9270868.png) | ![im1](c_plots/9270989.png) | ![im1](c_plots/9270997.png) \n",
    "\n",
    "So at this point I decided to increase the input-image size and start scaling images to 64x64 and I've introduced model `conv4` with it. It does increased my best score by 1%. It was considerably slower then the base model `cifar`(10x). Therefore I wanted to measure whether the increase was due to the increased #filters or due to the dropout layer. I've created two models while decreasing the filter sizes and making the models more plausable to train. \n",
    "\n",
    "Dropout appeared as being quite important for better accuracy. With the better and faster model `conv6` I've run 100 epoch and got 2.9%\n",
    "- **BEST_ERROR: ** around 2.9% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv7 and norm64\n",
    "![im1](bds/conv7.jpg) \n",
    "\n",
    "Then I've implemented couple of suggested normalization method. First I transformed the images to YUV space and then globally normalized them such that each channel has zero mean and unit variance. I've also extented `conv6` model with a starting Spatial Contrastive Normalization layer and named the new model suprizingly as `conv7`. I haven't obverved any significant difference between `norm64`(3.2%) and `basic64`(3%) preprocessing scripts. I've start observing oscillations at test error at this point and start thinking of implementing adaptive learning rate.\n",
    "\n",
    "conv6/basic64 | conv6/norm64 \n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9275422.png) | ![im1](c_plots/9275084.png) \n",
    "\n",
    "However `conv7` gave superior result to `conv6` and increased my best result to %97.4\n",
    "\n",
    "conv7/basic64 | conv7/norm64 \n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9277101.png) | ![im1](c_plots/9277119.png) \n",
    "\n",
    "- **BEST_ERROR: ** around 2.6% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# conv48-1 and cut48/cut48contra\n",
    "![im1](bds/conv48-1.jpg) \n",
    "\n",
    "So the deadline was approaching and I want to try more models and try them faster. So I decided decrease my input size to 48. I have adapted the `conv6` to the new input size and named as `conv48-1` (finally a better naming convention). Then I've also decided to implement cropping the images(rectangles are provided with data). I've included YUV-transformation and global normalizing as default to `cut48`. I've decided to implement Spatial Contrastive Normalization during preprocessing to prevent repeatitive processing and named the new as `cut48contra`. I've got 0.5% better results with `cut48contra` then `cut48` having the following convergence graphs with learning rate=0.05. \n",
    "\n",
    "conv48-1/cut48 (97.2%) | conv48-1/cut48contra (97.7%)\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9277617.png) | ![im1](c_plots/9277619.png) \n",
    "\n",
    "After this I've exprimented with learning rare and got worst results with LR=0.01. Then I've turned resampling on and got a slight improvemnt after 50 epochs ending up the best performance so far\n",
    "\n",
    "- **BEST_ERROR: ** around 2.2% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv48-2/3\n",
    "cconv48-2 | conv48-3\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](bds/conv48-2.jpg)| ![im1](bds/conv48-3.jpg)\n",
    "\n",
    "At this point I've decided implementing learning rate decay (since it became much more important at this point, error is so small). Along with that, I realized that I did pretty much enough at preprocessing side and decided focusing on the model and got inspired by the VGG-model. I've adapted the VGG idea into a new model and ended up with my biggest model `conv48-2`. Then I've decided to replace last 3 VGG-layer with a fully connected layer and reduce the #parameters(by 7) and #operations (by 3). I've did a lot of experiments with these two model and also with Learning Rate Decay(LRD). My findings being:\n",
    "- I've got similar results with two models. One reason being I didn't train `conv48-2` more than 70 epoch(1 epoch was taking 15min). \n",
    "- I  trained 150 epochs\n",
    "- LRD=0.1 didn't give good results. I think it is two big. I've got better results when I decreased LRD.\n",
    "- Resampling didn't give significant better results for these models. (One problem being, due to my implementation the dataset sizes are different and trainings become incomparable in terms of convergence and #epochs)\n",
    "- I got my best result in Kaggle with `conv48-3` following parameters LR=0.05, LRD=1e-4, without resampling\n",
    "\n",
    "Below I share convergence graphs for the two models:\n",
    "\n",
    "conv48-2 | conv48-3\n",
    ":-------------------------:|:-------------------------:\n",
    "![im1](c_plots/9279990.png) | ![im1](c_plots/9277622.png)\n",
    "\n",
    "- **BEST_ERROR: ** around 0.9% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
